{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/pleung/projects/bistable_bundle/r4/hinges\n",
      "dig28\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "# python internal\n",
    "import collections\n",
    "import copy\n",
    "import gc\n",
    "from glob import glob\n",
    "import h5py\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import socket\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# conda/pip\n",
    "import dask\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# special packages on the DIGS\n",
    "import py3Dmol\n",
    "import pymol\n",
    "import pyrosetta\n",
    "\n",
    "# notebook magic\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd())\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flo's original approach\n",
    "1) \n",
    "I used `/home/flop/switch/5thround/DHRs/make_DHR_states_4.ipynb` to generate states.  \n",
    "outputs should be in `/home/flop/switch/5thround/DHRs/states` and in `/home/flop/switch/5thround/DHRs/states_no_clashcheck/`  \n",
    "if you look at the notebook you'll see that there is a clash check option.  \n",
    "For the first couple of rounds discarded everything that failed the clash check, for the last round I ignored the clash check  \n",
    "mainly because in the beginning I used fixed bb as long as possible, later i switched to flex bb  \n",
    "and found that flexxbb design can actually produce very nice interfaces from starting points that clash heavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will follow Flo's docking procedure nearly exactly\n",
    "I will use the serialization build of PyRosetta to enable recording user defined info about the docks.  \n",
    "This enables downstream inline filtering and data analysis, as well as clustering by lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make functions for docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosetta.distributed.packed_pose.core import PackedPose\n",
    "from typing import *\n",
    "\n",
    "\n",
    "def load_from_silents(silent=None, **kwargs) -> Generator[str, PackedPose, None]:\n",
    "    \"\"\"\n",
    "    Generate PackedPose objects from a binary silent file.\n",
    "    Must initialize Rosetta with the \"-in:file:silent_struct_type binary\" flag.\n",
    "    \"\"\"\n",
    "    import pyrosetta.distributed.io as io\n",
    "\n",
    "    if silent == None:\n",
    "        silent = kwargs[\"-in:file:silent\"]\n",
    "    pposes = io.poses_from_silent(silent)\n",
    "    for ppose in pposes:\n",
    "        yield ppose\n",
    "\n",
    "\n",
    "def make_states(pose: PackedPose, **kwargs) -> list:\n",
    "    \"\"\"\n",
    "    Method to make a list of alternative states from the input PackedPose.\n",
    "    This is done by splitting, superimposing and rotating one full heptad up and one\n",
    "    full heptad down for helices before and after the break, defined by the\n",
    "    pre_break_helix kwarg.\n",
    "    \"\"\"\n",
    "    # TODO startup pyrosetta distributed? does pose need to be unpacked?\n",
    "    import pyrosetta\n",
    "    from pyrosetta.rosetta.core.pose import Pose\n",
    "    from pyrosetta.distributed.packed_pose.core import PackedPose\n",
    "    import pyrosetta.distributed.io as io\n",
    "\n",
    "    def range_CA_align(pose_a, pose_b, start_a, end_a, start_b, end_b):\n",
    "        \"\"\"\n",
    "        Align poses by superimposition of CA given two ranges of indices.\n",
    "        (pose 1 mobile)\n",
    "        Modified from apmoyer.\n",
    "        \"\"\"\n",
    "        import pyrosetta\n",
    "\n",
    "        pose_a_residue_selection = range(start_a, end_a)\n",
    "        pose_b_residue_selection = range(start_b, end_b)\n",
    "\n",
    "        assert len(pose_a_residue_selection) == len(pose_b_residue_selection)\n",
    "\n",
    "        pose_a_coordinates = (\n",
    "            pyrosetta.rosetta.utility.vector1_numeric_xyzVector_double_t()\n",
    "        )\n",
    "        pose_b_coordinates = (\n",
    "            pyrosetta.rosetta.utility.vector1_numeric_xyzVector_double_t()\n",
    "        )\n",
    "\n",
    "        for pose_a_residue_index, pose_b_residue_index in zip(\n",
    "            pose_a_residue_selection, pose_b_residue_selection\n",
    "        ):\n",
    "            pose_a_coordinates.append(pose_a.residues[pose_a_residue_index].xyz(\"CA\"))\n",
    "            pose_b_coordinates.append(pose_b.residues[pose_b_residue_index].xyz(\"CA\"))\n",
    "\n",
    "        rotation_matrix = pyrosetta.rosetta.numeric.xyzMatrix_double_t()\n",
    "        pose_a_center = pyrosetta.rosetta.numeric.xyzVector_double_t()\n",
    "        pose_b_center = pyrosetta.rosetta.numeric.xyzVector_double_t()\n",
    "\n",
    "        pyrosetta.rosetta.protocols.toolbox.superposition_transform(\n",
    "            pose_a_coordinates,\n",
    "            pose_b_coordinates,\n",
    "            rotation_matrix,\n",
    "            pose_a_center,\n",
    "            pose_b_center,\n",
    "        )\n",
    "\n",
    "        pyrosetta.rosetta.protocols.toolbox.apply_superposition_transform(\n",
    "            pose_a, rotation_matrix, pose_a_center, pose_b_center\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def clash_check(pose: Pose) -> float:\n",
    "        \"\"\"\n",
    "        Mutate all residues to glycine then check the score of the mutated pose.\n",
    "        Print the score and return it.\n",
    "        \"\"\"\n",
    "        import pyrosetta\n",
    "\n",
    "        # initialize empty sfxn\n",
    "        sfxn = pyrosetta.rosetta.core.scoring.ScoreFunction()\n",
    "        sfxn.set_weight(pyrosetta.rosetta.core.scoring.fa_rep, 1)\n",
    "        # make the pose into a backbone without sidechains\n",
    "        all_gly = pose.clone()\n",
    "        true_sel = pyrosetta.rosetta.core.select.residue_selector.TrueResidueSelector()\n",
    "        true_x = true_sel.apply(all_gly)\n",
    "        # settle the pose\n",
    "        pyrosetta.rosetta.protocols.toolbox.pose_manipulation.repack_these_residues(\n",
    "            true_x, all_gly, sfxn, False, \"G\"\n",
    "        )\n",
    "        score = sfxn(all_gly)\n",
    "        return score\n",
    "\n",
    "    def helix_dict_maker(pose: Pose) -> dict:\n",
    "        \"\"\"\n",
    "        Make a dictionary of which residues belong to which helices then return it.\n",
    "        \"\"\"\n",
    "        import pyrosetta\n",
    "\n",
    "        ss = pyrosetta.rosetta.core.scoring.dssp.Dssp(pose)\n",
    "        helix_dict = {}\n",
    "        n = 1\n",
    "        for i in range(1, len(pose.sequence())):\n",
    "            if (ss.get_dssp_secstruct(i) == \"H\") & (\n",
    "                ss.get_dssp_secstruct(i - 1) != \"H\"\n",
    "            ):\n",
    "                helix_dict[n] = [i]\n",
    "            if (ss.get_dssp_secstruct(i) == \"H\") & (\n",
    "                ss.get_dssp_secstruct(i + 1) != \"H\"\n",
    "            ):\n",
    "                helix_dict[n].append(i)\n",
    "                n += 1\n",
    "        return helix_dict\n",
    "\n",
    "    def get_helix_endpoints(pose: Pose, n_terminal: bool) -> dict:\n",
    "        \"\"\"\n",
    "        Make a dictionary of the start (n_terminal=True) or end residue indices of each helix\n",
    "        \"\"\"\n",
    "        helix_dict = helix_dict_maker(pose)\n",
    "        helix_endpoints = {}\n",
    "        if n_terminal:\n",
    "            index = 0  # helix start residue\n",
    "        else:\n",
    "            index = -1  # helix end residue\n",
    "        for helix, residue_list in helix_dict.items():\n",
    "            helix_endpoints[helix] = residue_list[index]\n",
    "        return helix_endpoints\n",
    "\n",
    "    def combine_two_poses(pose_a: Pose, pose_b: Pose, end_a: int, start_b: int) -> Pose:\n",
    "        \"\"\"\n",
    "        Make a new pose, containing pose_a up to end_a, then pose_b starting from start_b\n",
    "        Assumes pose_a has only one chain.\n",
    "        \"\"\"\n",
    "        import pyrosetta\n",
    "        from pyrosetta.rosetta.core.pose import Pose\n",
    "\n",
    "        assert len(pose_a.sequence()) == len(pose_b.sequence())\n",
    "        length = len(pose_a.sequence())\n",
    "        newpose = Pose()\n",
    "        for i in range(1, end_a + 1):\n",
    "            newpose.append_residue_by_bond(pose_a.residue(i))\n",
    "        newpose.append_residue_by_jump(\n",
    "            pose_b.residue(start_b), newpose.chain_end(1), \"CA\", \"CA\", 1\n",
    "        )\n",
    "        for i in range(start_b + 1, length + 1):\n",
    "            newpose.append_residue_by_bond(pose_b.residue(i))\n",
    "        return newpose\n",
    "\n",
    "    def shift_chB_by_i(\n",
    "        pose: Pose,\n",
    "        i: int,\n",
    "        starts: dict,\n",
    "        ends: dict,\n",
    "        pivot_helix: int,\n",
    "        pre_break_helix: int,\n",
    "    ) -> Pose:\n",
    "        import pyrosetta\n",
    "        from pyrosetta.rosetta.core.pose import Pose\n",
    "\n",
    "        pose = pose.clone()\n",
    "        copypose = pose.clone()\n",
    "        start = starts[pivot_helix]\n",
    "        end = ends[pivot_helix]\n",
    "        starts_tup = tuple(start for start in 4 * [start])\n",
    "        ends_tup = tuple(end for end in 4 * [end])\n",
    "        # make sure there's enough helix to align against going forwards\n",
    "        if (i >= 0) and ((start + 10 + i) <= end):\n",
    "            offsets = 3, 10, 3 + i, 10 + i\n",
    "            start_a, end_a, start_b, end_b = tuple(map(sum, zip(starts_tup, offsets)))\n",
    "        # make sure there's enough helix to align against going backwards\n",
    "        elif (i <= 0) and ((end - 10 + i) >= start):\n",
    "            offsets = -10, -3, -10 + i, -3 + i\n",
    "            start_a, end_a, start_b, end_b = tuple(map(sum, zip(ends_tup, offsets)))\n",
    "        else:\n",
    "            raise RuntimeError(\"not enough overlap to align\")\n",
    "        range_CA_align(copypose, pose, start_a, end_a, start_b, end_b)\n",
    "        end_pose_a, start_pose_b = ends[pre_break_helix], starts[pre_break_helix + 1]\n",
    "        shifted_pose = combine_two_poses(pose, copypose, end_pose_a, start_pose_b)\n",
    "        return shifted_pose\n",
    "\n",
    "    pose = io.to_pose(pose)\n",
    "    if not \"name\" in kwargs:\n",
    "        name = pose.pdb_info().name()\n",
    "    else:\n",
    "        name = kwargs[\"name\"]\n",
    "    try:\n",
    "        pre_break_helix = kwargs[\"pre_break_helix\"]\n",
    "    except KeyError:\n",
    "        raise RuntimeError(\"need to supply pre_break_helix\")\n",
    "    parent_length = len(pose.residues)\n",
    "    starts = get_helix_endpoints(pose, n_terminal=True)\n",
    "    ends = get_helix_endpoints(pose, n_terminal=False)\n",
    "    states = []\n",
    "    nstruct = 0\n",
    "    post_break_helix = pre_break_helix + 1\n",
    "    # scan 1 heptad forwards and backwards\n",
    "    for i in range(-7, 8):\n",
    "        # first do the pre break side, then do the post break side\n",
    "        for pivot_helix in [pre_break_helix, post_break_helix]:\n",
    "            try:\n",
    "                shift = shift_chB_by_i(\n",
    "                    pose, i, starts, ends, pivot_helix, pre_break_helix\n",
    "                )\n",
    "                nstruct += 1\n",
    "                pdb_info = pyrosetta.rosetta.core.pose.PDBInfo(shift)\n",
    "                shift.pdb_info(pdb_info)\n",
    "                shift.pdb_info().name(name + \"_{}\".format(nstruct))\n",
    "                bb_clash = clash_check(shift)\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(\n",
    "                    shift, \"bb_clash\", float(bb_clash)\n",
    "                )\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(shift, \"parent\", name)\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(\n",
    "                    shift, \"parent_length\", str(parent_length)\n",
    "                )\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(\n",
    "                    shift, \"pivot_helix\", str(pivot_helix)\n",
    "                )\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(\n",
    "                    shift, \"pre_break_helix\", str(pre_break_helix)\n",
    "                )\n",
    "                pyrosetta.rosetta.core.pose.setPoseExtraScore(shift, \"shift\", str(i))\n",
    "                ppose = io.to_packed(shift)\n",
    "                states.append(ppose)\n",
    "            except:  # for cases where there isn't enough to align against\n",
    "                continue\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dask\n",
    "Trying a adaptive SLURMCluster. to see the dashboard, forward port `8787` to `8000`:  \n",
    "`local$ ssh -L 8000:localhost:8787 $USER@$HOSTNAME`  \n",
    "now, the web UI is visible at `localhost:8000`  \n",
    "if you're using a local cluster make sure the node this notebook is on has the same \n",
    "number of workers as cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dig28\n",
      "pleung\n",
      "run one of the following from your local terminal:\n",
      "ssh -L 8000:localhost:8787 pleung@dig28\n",
      "ssh -L 8000:localhost:8787 pleung@dig28.ipd\n"
     ]
    }
   ],
   "source": [
    "!echo $HOSTNAME\n",
    "!echo $USER\n",
    "\n",
    "import pwd\n",
    "\n",
    "print(\"run one of the following from your local terminal:\")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}\"\n",
    ")\n",
    "print(\n",
    "    f\"ssh -L 8000:localhost:8787 {pwd.getpwuid(os.getuid()).pw_name}@{socket.gethostname()}.ipd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e /mnt/home/pleung/logs/slurm_logs/dask-worker-%J.err\n",
      "#SBATCH -o /mnt/home/pleung/logs/slurm_logs/dask-worker-%J.out\n",
      "#SBATCH -p short\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=4G\n",
      "#SBATCH -t 1:30:00\n",
      "\n",
      "JOB_ID=${SLURM_JOB_ID%;*}\n",
      "\n",
      "/home/pleung/.conda/envs/phil/bin/python -m distributed.cli.dask_worker tcp://172.16.131.58:44801 --nthreads 1 --memory-limit 4.00GB --name name --nanny --death-timeout 999 --local-directory $TMPDIR/dask --lifetime 55m --lifetime-stagger 4m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.16.131.58:44801</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.16.131.58:8787/status' target='_blank'>http://172.16.131.58:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.16.131.58:44801' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    cores=1,\n",
    "    processes=1,\n",
    "    job_cpu=1,\n",
    "    memory=\"4GB\",\n",
    "    queue=\"short\",\n",
    "    walltime=\"1:30:00\",\n",
    "    death_timeout=999,\n",
    "    local_directory=\"$TMPDIR/dask\",\n",
    "    log_directory=\"/mnt/home/pleung/logs/slurm_logs\",\n",
    "    extra=[\"--lifetime\", \"55m\", \"--lifetime-stagger\", \"4m\"],\n",
    ")\n",
    "print(cluster.job_script())\n",
    "# setup for standard,\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 9999})\n",
    "# scale between 1-100 workers,\n",
    "cluster.adapt(minimum=1, maximum=100, wait_count=999, interval=\"5s\")\n",
    "# spinup client with cluster\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set command line options, make tasks and submit to client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 'pyrosetta.distributed.cluster.converters' requires the third-party packages 'cloudpickle', 'gitpython', and 'toolz' as dependencies!\n",
      "Please install these packages into your python environment. For installation instructions, visit:\n",
      "https://pypi.org/project/cloudpickle/\n",
      "https://gitpython.readthedocs.io/en/stable/intro.html\n",
      "https://pypi.org/project/toolz/\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'git'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7b100240ba74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyRosettaCluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phil/lib/python3.8/site-packages/pyrosetta/distributed/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyRosettaCluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m from pyrosetta.distributed.cluster.toolkit import (\n\u001b[1;32m     17\u001b[0m     \u001b[0mget_instance_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phil/lib/python3.8/site-packages/pyrosetta/distributed/cluster/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaskBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_residue_type_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m from pyrosetta.distributed.cluster.converters import (\n\u001b[1;32m    222\u001b[0m     \u001b[0m_parse_decoy_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phil/lib/python3.8/site-packages/pyrosetta/distributed/cluster/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_parse_protocols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m from pyrosetta.distributed.cluster.initialization import (\n\u001b[1;32m     21\u001b[0m     \u001b[0m_get_residue_type_set_name3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_residue_type_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phil/lib/python3.8/site-packages/pyrosetta/distributed/cluster/converters.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mgit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtoolz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'git'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pyrosetta.distributed.io as io\n",
    "from pyrosetta.distributed.cluster.core import PyRosettaCluster\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "silents = glob(os.path.join(os.getcwd(), \"00_silents/*.silent\"))\n",
    "\n",
    "print(\"silents: {}\".format(\",\".join(silents)))\n",
    "\n",
    "options = {\n",
    "    \"-out:level\": \"300\",\n",
    "    \"-in:file:silent_struct_type\": \"binary\",\n",
    "}\n",
    "\n",
    "def create_tasks(options):\n",
    "    for silent in silents:\n",
    "        tasks = {\n",
    "            \"options\": \"\",\n",
    "            \"extra_options\": options,\n",
    "            \"set_logging_handler\": \"interactive\",\n",
    "            \"-in:file:silent\": os.path.join(os.getcwd(),silent),\n",
    "        }\n",
    "        # use kwargs to match the right pre_break_helix with the right scaffold type\n",
    "        if \"thr\" in silent:\n",
    "            tasks[\"pre_break_helix\"] = 6\n",
    "        else:\n",
    "            tasks[\"pre_break_helix\"] = 4\n",
    "        yield tasks\n",
    "        \n",
    "if not os.getenv(\"DEBUG\"):\n",
    "    output_path = os.path.join(os.getcwd(), \"01_make_states\")\n",
    "    PyRosettaCluster(\n",
    "        tasks=create_tasks(options),\n",
    "        client=client,\n",
    "        scratch_dir=output_path,\n",
    "        output_path=output_path,\n",
    "    ).distribute(protocols=[load_from_silents, make_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at scores\n",
    "There is certainly a less embarrassing way to do this but at least this way is vectorized, so it should scale very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scorefile(scores):\n",
    "    import pandas as pd\n",
    "    scores = pd.read_json(scores, orient=\"records\", typ=\"frame\", lines=True)\n",
    "    scores = scores.T\n",
    "    mat = scores.values\n",
    "    n = mat.shape[0]\n",
    "    dicts = list(mat[range(n), range(n)])\n",
    "    index = scores.index\n",
    "    tabulated_scores = pd.DataFrame(dicts, index=index)\n",
    "    return tabulated_scores\n",
    "    \n",
    "output_path = os.path.join(os.getcwd(), \"01_make_states\")\n",
    "scores = os.path.join(output_path, \"scores.json\")\n",
    "scores_df = read_scorefile(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze some scoreterms\n",
    "Figure out a reasonable cutoff for filtering perhaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "sns.set(\n",
    "    context=\"talk\",\n",
    "    font_scale=2, # make the font larger; default is pretty small\n",
    "    style=\"ticks\", # make the background white with black lines\n",
    "    palette=\"colorblind\" # a color palette that is colorblind friendly!\n",
    ")\n",
    "\n",
    "order = [int(x) for x in set(scores_df[\"shift\"].values)]\n",
    "order.sort()\n",
    "order = [str(x) for x in order]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10), tight_layout=True)\n",
    "pivot_order = ['4', '5', '6', '7']\n",
    "ax = sns.boxplot(\n",
    "    x=\"shift\", y=\"bb_clash\", data=scores_df,\n",
    "    order= order, showfliers=False,\n",
    "    hue=\"pivot_helix\", hue_order=pivot_order,\n",
    ")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_ylim(0, 250000)\n",
    "sns.despine()\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='24')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(\"figs/01_shift_vs_clash_groupby_pivot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parents = sorted(list(set(scores_df.parent.values)))\n",
    "thrs = [x for x in sorted_parents if \"THR\" in x]\n",
    "dhrs_nocys = [x for x in sorted_parents if \"nocys\" in x]\n",
    "th_dhrs = [x for x in sorted_parents if \"TH_DHR\" in x]\n",
    "kh = [x for x in sorted_parents if \"KH_\" in x]\n",
    "dhrs = [x for x in sorted_parents if (\"THR\" not in x and \"nocys\" not in x and \"TH_DHR\" not in x and \"KH_\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orders = [dhrs, dhrs_nocys, th_dhrs, thrs]\n",
    "fig, axs = plt.subplots(2, 2, sharey=True, figsize=(30,30))\n",
    "fig.suptitle(\"Parent vs. Clash\")\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "for order, ax in zip(data_orders, axs.flatten()):\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "    ax.set_ylim(0, 250000)\n",
    "    sns.boxplot(\n",
    "        ax=ax,\n",
    "        x=\"parent\", y=\"bb_clash\", data=scores_df,\n",
    "        order=order, showfliers = False,\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        ax=ax,\n",
    "        x=\"parent\", y=\"bb_clash\", data=scores_df,\n",
    "        order=order,\n",
    "    )\n",
    "    sns.despine()\n",
    "axs[0][0].set_title(\"DHRS\"); axs[0][1].set_title(\"no_cys\")\n",
    "axs[1][0].set_title(\"TH_DHRS\"); axs[1][1].set_title(\"THRS\")\n",
    "plt.savefig(\"figs/01_all_parents_vs_clash.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orders = [dhrs, dhrs_nocys, th_dhrs, thrs]\n",
    "pivot_orders = [['4', '5'], ['4', '5'], ['4', '5'], ['6', '7']]\n",
    "fig, axs = plt.subplots(2, 2, sharey=True, figsize=(30,30))\n",
    "fig.suptitle(\"Parent vs. Clash, Subgrouped by Pivot Order\")\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "for order, ax, pivot_order in zip(data_orders, axs.flatten(), pivot_orders):\n",
    "    ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "    ax.set_ylim(0, 250000)\n",
    "    sns.boxplot(\n",
    "        ax=ax,\n",
    "        x=\"parent\", y=\"bb_clash\", data=scores_df,\n",
    "        order=order, showfliers = False,\n",
    "        hue=\"pivot_helix\", hue_order=pivot_order,\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        ax=ax,\n",
    "        x=\"parent\", y=\"bb_clash\", data=scores_df,\n",
    "        order=order,\n",
    "        hue=\"pivot_helix\", hue_order=pivot_order,\n",
    "    )\n",
    "    sns.despine()\n",
    "    ax.legend(loc=\"upper right\")\n",
    "axs[0][0].set_title(\"DHRS\"); axs[0][1].set_title(\"no_cys\")\n",
    "axs[1][0].set_title(\"TH_DHRS\"); axs[1][1].set_title(\"THRS\")\n",
    "plt.savefig(\"figs/01_all_parents_vs_clash_subgrouped_pivots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do more analysis\n",
    "First group into major scaffold classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaffold = []\n",
    "for value in scores_df.parent.values:\n",
    "    if \"TH_DHR\" in value:\n",
    "        name = \"TH_DHR\"\n",
    "    elif \"KH_\"  in value:\n",
    "        name = \"KH_DHR\"\n",
    "    elif \"THR\" in value:\n",
    "        name = \"THR\"\n",
    "    elif \"DHR\" in value:\n",
    "        name = \"DHR\"\n",
    "    else:\n",
    "        print(\"unexpected name encountered\")\n",
    "    scaffold.append(name)\n",
    "scores_df[\"scaffold\"] = scaffold\n",
    "scores_df.groupby([\"scaffold\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = list(sns.color_palette(\"colorblind\", 256).as_hex())\n",
    "palette = palette[:4]\n",
    "scaffold_classes = sorted(list(set(scores_df.scaffold.values)))\n",
    "color_dict = dict(zip(scaffold_classes, palette))\n",
    "\n",
    "x = [int(x) for x in scores_df[\"pivot_helix\"]]\n",
    "y = [int(x) for x in scores_df[\"shift\"]]\n",
    "z = list(scores_df[\"bb_clash\"])\n",
    "c = [color_dict[x] for x in list(scores_df[\"scaffold\"])]\n",
    "\n",
    "fig = plt.figure(figsize=(20,20), tight_layout=True)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "sc = ax.scatter(x, y, z, c=c, s=100, alpha=0.7)\n",
    "ax.set_xlabel(\"pivot\", labelpad=100)\n",
    "ax.set_ylabel(\"shift\", labelpad=100)\n",
    "ax.set_zlabel(\"clash\", labelpad=100)\n",
    "ax.set_zlim(0,250000)\n",
    "ax.tick_params(axis=\"both\", labelrotation=-45)\n",
    "ax.set_xticks([4,5,6,7])\n",
    "ax.set_yticks(range(-7,8))\n",
    "markers = [plt.Line2D([0,0],[0,0],color=color, marker=\"o\", markersize=20, linestyle=\"\") for color in color_dict.values()]\n",
    "plt.legend(markers, color_dict.keys(), numpoints=1)\n",
    "fig.suptitle(\"Pivot vs. Shift vs. Clash vs. Scaffold\")\n",
    "plt.savefig(\"figs/01_pivot_vs_shift_vs_clash_subgrouped_scaffolds.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "1. THRs have more variability in clash but higher overall clash, and it varies a lot more as a function of the shift  \n",
    "2. DHRs vary more between parent, as would be expected for their greater backbone diversity\n",
    "3. TH_DHRs are similar in metrics to THRs\n",
    "3. KH_DHRs are similar in metrics to DHRs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get decoys that pass a bb_clash cutoff, determine the best cutoff\n",
    "my pilot runs seemed to indicate that flo was right, starting `bb_clash` score doesnt seem to be predictive of final interface quality...   \n",
    "... at least `score_per_res` anyway. David thinks could move the cutoff lower but we can always filter that out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs, num = [], []\n",
    "fig = plt.figure(figsize=(15,10), tight_layout=True)\n",
    "\n",
    "for cutoff in range(50, 500000, 50):\n",
    "    filtered = scores_df[scores_df[\"bb_clash\"] < cutoff]\n",
    "    cutoffs.append(cutoff), num.append(len(filtered))\n",
    "    \n",
    "ax = sns.lineplot(x=cutoffs, y=num)\n",
    "sns.despine()\n",
    "ax.set_xscale(\"log\")\n",
    "_ = plt.xticks([100,1000,10000,100000])\n",
    "plt.xlabel(\"bb_clash cutoff\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.suptitle(\"Count included as a function of cutoff\")\n",
    "plt.savefig(\"figs/01_cutoff_vs_count.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think a flat cutoff of around 20000 should be fine \n",
    "Time to filter and pack the poses into a silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = scores_df[scores_df[\"bb_clash\"] < 20000]\n",
    "print(len(selected), \"selected structures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10), tight_layout=True)\n",
    "scores_df.groupby(\"scaffold\").size().plot(kind=\"pie\", autopct=\"%1.2f%%\", ax=ax1)\n",
    "selected.groupby(\"scaffold\").size().plot(kind=\"pie\", autopct=\"%1.2f%%\", ax=ax2)\n",
    "ax1.set_ylabel(\"Pre selection\")\n",
    "ax2.set_ylabel(\"Post selection\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/01_pre_post_selection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel unzipping, relabeling and packing into a silent of all selected designs\n",
    "Method maintains score info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrosetta.distributed.io as io\n",
    "from pyrosetta.distributed.packed_pose.core import PackedPose\n",
    "\n",
    "def unpack_add_scores_repack(bz2file:str, scores:dict) -> PackedPose:\n",
    "    import bz2\n",
    "    import os\n",
    "    import pyrosetta\n",
    "    import pyrosetta.distributed.io as io\n",
    "    with open(bz2file, \"rb\") as f:\n",
    "        ppose = io.pose_from_pdbstring(bz2.decompress(f.read()).decode())\n",
    "    pose = io.to_pose(ppose)\n",
    "    basename =  os.path.basename(bz2file.replace(\".pdb.bz\", \"\", 1))\n",
    "    pose.pdb_info().name(basename)\n",
    "    for key, value in scores.items():\n",
    "        pyrosetta.rosetta.core.pose.setPoseExtraScore(pose, key, value)\n",
    "    final_ppose = io.to_packed(pose)\n",
    "    return final_ppose\n",
    "\n",
    "if not os.getenv(\"DEBUG\"):\n",
    "    future_pposes = []\n",
    "    for i, bz2file in enumerate(selected.index, start=1):\n",
    "        # decompress from bz2, add score info and pack\n",
    "        scores_dict = dict(selected.loc[bz2file])\n",
    "        future_ppose = client.submit(unpack_add_scores_repack, bz2file, scores_dict)\n",
    "        future_pposes.append(future_ppose)\n",
    "    pposes = []\n",
    "    for future in tqdm(future_pposes):\n",
    "        pposes.append(future.result())\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), \"01_silents\")\n",
    "silent_name = \"states.silent\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "io.to_silent(pposes, os.path.join(out_path, silent_name))\n",
    "msg = \"\"\"\n",
    "out_path: {out_path}\n",
    "packed {i} poses\n",
    "\"\"\".format(out_path=out_path, i=i)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperately collect parent reference states\n",
    "Parents would have a `shift` of 0 and a `pivot_helix` of 4 or 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = scores_df.query(\"shift == '0' & (pivot_helix == '4' | pivot_helix == '6')\")\n",
    "print(len(ref), \"reference structures\")\n",
    "\n",
    "if not os.getenv(\"DEBUG\"):\n",
    "    future_pposes = []\n",
    "    for i, bz2file in enumerate(ref.index, start=1):\n",
    "        # decompress from bz2, add score info and pack\n",
    "        scores_dict = dict(ref.loc[bz2file])\n",
    "        future_ppose = client.submit(unpack_add_scores_repack, bz2file, scores_dict)\n",
    "        future_pposes.append(future_ppose)\n",
    "    pposes = []\n",
    "    for future in tqdm(future_pposes):\n",
    "        pposes.append(future.result())\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), \"01_silents\")\n",
    "silent_name = \"reference.silent\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "io.to_silent(pposes, os.path.join(out_path, silent_name))\n",
    "msg = \"\"\"\n",
    "out_path: {out_path}\n",
    "packed {i} poses\n",
    "\"\"\".format(out_path=out_path, i=i)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phil (3.8.2)",
   "language": "python",
   "name": "phil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
